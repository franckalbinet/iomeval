# IOMEval


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

[![CI](https://github.com/franckalbinet/iomeval/actions/workflows/test.yaml/badge.svg)](https://github.com/franckalbinet/iomeval/actions/workflows/test.yaml)
[![PyPI](https://img.shields.io/pypi/v/iomeval.png)](https://pypi.org/project/iomeval/)

## Installation

``` sh
pip install iomeval
```

## Configuration

iomeval uses Claude for intelligent extraction and mapping. Set your
Anthropic API key:

``` sh
export ANTHROPIC_API_KEY='your-key-here'
```

add mistral also …

## Key Features

- **Automated PDF Processing**: Download and OCR evaluation reports with
  proper heading hierarchy
- **Intelligent Section Extraction**: LLM-powered extraction of
  executive summaries, findings, conclusions, and recommendations
- **Strategic Framework Mapping**: Map report content to IOM’s SRF
  Enablers, Cross-Cutting Priorities, GCM Objectives, and SRF Outputs
- **Checkpoint/Resume**: Built-in state persistence - interrupt and
  resume long-running pipelines
- **Granular Control**: Use the full pipeline or individual components
  as needed

## Quick Start

Process an evaluation report end-to-end:

``` python
from iomeval.pipeline import Report, run_pipeline

report = Report(id="IOM-2024-001", pdf_url="evaluation_report.pdf")
run_pipeline(report)
```

The rich display shows processing status across all pipeline stages -
from OCR through framework mapping.

## Understanding Evaluation Mapping in the UN Context

UN agencies conduct hundreds of evaluations annually. Each report
contains valuable insights about what works (and what doesn’t) in
international development and humanitarian response. However, these
insights are often:

- Buried in lengthy PDF documents
- Difficult to compare across projects
- Hard to connect to strategic frameworks and goals

iomeval addresses this by automatically extracting key sections and
mapping findings to IOM’s strategic frameworks, enabling:

- **Portfolio Analysis**: Understand patterns across multiple
  evaluations
- **Strategic Alignment**: See how project outcomes connect to
  organizational priorities
- **Knowledge Management**: Make evaluation insights searchable and
  comparable

## Detailed Workflow

For more control, use individual pipeline stages:

### 1. Load evaluation metadata

``` python
from iomeval.readers import read_iom

evals = read_iom()  # Returns DataFrame of all IOM evaluations
evals.head()
```

### 2. Download and OCR a report

``` python
from iomeval.downloaders import download_pdf
from iomeval.core import pdf_to_md

# Download PDF
pdf_path = download_pdf(report.pdf_url, dest_folder="pdfs")

# Convert to markdown with heading hierarchy
report.md = pdf_to_md(pdf_path)
```

### 3. Extract key sections

``` python
from iomeval.extract import extract_exec_summary, extract_findings, extract_conclusions, extract_recommendations

report.exec_summary = extract_exec_summary(report.md)
report.findings = extract_findings(report.md)
report.conclusions = extract_conclusions(report.md)
report.recommendations = extract_recommendations(report.md)
```

### 4. Map to strategic frameworks

``` python
from iomeval.mapper import map_srf_enablers, map_srf_crosscutting, map_gcm, map_srf_outputs

# Each mapper returns structured results with centrality scores
report.srf_enablers = map_srf_enablers(report.conclusions)
report.srf_crosscutting = map_srf_crosscutting(report.conclusions)
report.gcm = map_gcm(report.conclusions)
report.srf_outputs = map_srf_outputs(report.conclusions)
```

### 5. Save/restore checkpoints

``` python
# Save progress
report.save("checkpoint.json")

# Resume later
report = Report.load("checkpoint.json")
```

## Development

iomeval is built with [nbdev](https://nbdev.fast.ai/), which means the
entire library is developed in Jupyter notebooks. The notebooks serve as
both documentation and source code.

### Setup for development

``` sh
git clone https://github.com/franckalbinet/iomeval.git
cd iomeval
pip install -e '.[dev]'
```

### Key nbdev commands

``` sh
nbdev_test          # Run tests in notebooks
nbdev_export        # Export notebooks to Python modules
nbdev_preview       # Preview documentation site
nbdev_prepare       # Export, test, and clean notebooks (run before committing)
```

### Workflow

1.  Make changes in the `.ipynb` notebook files
2.  Run `nbdev_prepare` to export code and run tests
3.  Commit both notebooks and exported Python files
4.  Documentation is automatically generated from the notebooks

Learn more about nbdev’s literate programming approach in the [nbdev
documentation](https://nbdev.fast.ai/).

### Contributing

Contributions are welcome! Please: - Follow the existing notebook
structure - Add tests using nbdev’s `#| test` cells - Run
`nbdev_prepare` before submitting PRs
