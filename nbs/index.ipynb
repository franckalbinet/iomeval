{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc76840",
   "metadata": {},
   "source": [
    "# IOMEval\n",
    "> Streamline evaluation reports into structured knowledge maps with LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d187f1",
   "metadata": {},
   "source": [
    "[![CI](https://github.com/franckalbinet/iomeval/actions/workflows/test.yaml/badge.svg)](https://github.com/franckalbinet/iomeval/actions/workflows/test.yaml)\n",
    "[![PyPI](https://img.shields.io/pypi/v/iomeval)](https://pypi.org/project/iomeval/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e6e49a",
   "metadata": {},
   "source": [
    "`iomeval` streamlines the mapping of [IOM](https://www.iom.int) evaluation reports against strategic frameworks like the [Strategic Results Framework (SRF)](https://srf.iom.int/) and [Global Compact for Migration (GCM)](https://www.un.org/en/development/desa/population/migration/generalassembly/docs/globalcompact/A_RES_73_195.pdf). It uses LLMs to process PDF reports, extract key sections, and tag/map them to framework components, turning dispersed, untagged evaluation documents into structured, searchable knowledge maps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12ae036",
   "metadata": {},
   "source": [
    "## Why This Matters\n",
    "\n",
    "UN agencies produce extensive evaluation reports and other public documents. For IOM, this body of knowledge is extensive and variegated, but putting it to practical use becomes more challenging as volume increases, particularly when documentation is stored across different repositories with no single index available.\n",
    "\n",
    "**The Challenge for IOM**\n",
    "\n",
    "IOM's evaluation production is highly decentralized, with reports stored across multiple repositories (the [IOM Evaluation Repository](https://evaluation.iom.int/evaluation-search-pdf), IOM Library, IOM Protection Platform). Quality varies greatly—quality control processes are not applied uniformly, and variation also reflects the inherent subjectivity in evaluation approaches and interpretations. Reports also vary significantly in structure: some follow common formats with executive summaries, findings, and recommendations, while others have different structures entirely. This inconsistency makes systematic mapping challenging.\n",
    "\n",
    "Critically, existing metadata doesn't indicate which elements of IOM's strategic frameworks—the [Strategic Results Framework (SRF)](https://srf.iom.int/) or the [Global Compact for Migration (GCM)](https://www.un.org/en/development/desa/population/migration/generalassembly/docs/globalcompact/A_RES_73_195.pdf)—each report addresses. This is a major gap that limits the ability to connect evaluation evidence with key strategic frameworks.\n",
    "\n",
    "**Evidence Maps as a Solution**\n",
    "\n",
    "Evidence Maps display the extent and nature of research and evaluation available on a subject. Following the 2025 UNEG Eval Week, four primary use cases emerged: guiding future evidence generation, informing policy decisions, knowledge management, and enhancing collaboration. The maps created by `iomeval` serve primarily as **knowledge management tools**—structured repositories that make identifying relevant sources easier by organizing them against strategic framework components.\n",
    "\n",
    "**What This Enables**\n",
    "\n",
    "By tagging reports against SRF outputs, enablers, cross-cutting priorities, and GCM objectives, these maps help answer questions like: Which framework elements are well-covered by existing evaluations? Where are the knowledge gaps that should prioritize future evaluation work? Which themes have enough evidence for a dedicated synthesis report?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9441fdd8",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "```sh\n",
    "pip install iomeval\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f48f0",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "iomeval uses Claude for intelligent extraction and mapping. Set your Anthropic API key:\n",
    "\n",
    "```sh\n",
    "export ANTHROPIC_API_KEY='your-key-here'\n",
    "```\n",
    "\n",
    "add mistral also ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256bff03",
   "metadata": {},
   "source": [
    "## Key Features\n",
    "\n",
    "- **Automated PDF Processing**: Download and OCR evaluation reports with proper heading hierarchy\n",
    "- **Intelligent Section Extraction**: LLM-powered extraction of executive summaries, findings, conclusions, and recommendations\n",
    "- **Strategic Framework Mapping**: Map report content to IOM's SRF Enablers, Cross-Cutting Priorities, GCM Objectives, and SRF Outputs\n",
    "- **Checkpoint/Resume**: Built-in state persistence - interrupt and resume long-running pipelines\n",
    "- **Granular Control**: Use the full pipeline or individual components as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58acdbb3",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "\n",
    "Process an evaluation report end-to-end:\n",
    "\n",
    "```python\n",
    "from iomeval.pipeline import Report, run_pipeline\n",
    "\n",
    "report = Report(id=\"IOM-2024-001\", pdf_url=\"evaluation_report.pdf\")\n",
    "run_pipeline(report)\n",
    "```\n",
    "\n",
    "The rich display shows processing status across all pipeline stages - from OCR through framework mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c424614",
   "metadata": {},
   "source": [
    "## Detailed Workflow\n",
    "\n",
    "For more control, use individual pipeline stages:\n",
    "\n",
    "### 1. Load evaluation metadata\n",
    "\n",
    "```python\n",
    "from iomeval.readers import read_iom\n",
    "\n",
    "evals = read_iom()  # Returns DataFrame of all IOM evaluations\n",
    "evals.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6950ab77",
   "metadata": {},
   "source": [
    "### 2. Download and OCR a report\n",
    "\n",
    "```python\n",
    "from iomeval.downloaders import download_pdf\n",
    "from iomeval.core import pdf_to_md\n",
    "\n",
    "# Download PDF\n",
    "pdf_path = download_pdf(report.pdf_url, dest_folder=\"pdfs\")\n",
    "\n",
    "# Convert to markdown with heading hierarchy\n",
    "report.md = pdf_to_md(pdf_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6ead15",
   "metadata": {},
   "source": [
    "### 3. Extract key sections\n",
    "\n",
    "```python\n",
    "from iomeval.extract import extract_exec_summary, extract_findings, extract_conclusions, extract_recommendations\n",
    "\n",
    "report.exec_summary = extract_exec_summary(report.md)\n",
    "report.findings = extract_findings(report.md)\n",
    "report.conclusions = extract_conclusions(report.md)\n",
    "report.recommendations = extract_recommendations(report.md)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e74e0",
   "metadata": {},
   "source": [
    "### 4. Map to strategic frameworks\n",
    "\n",
    "```python\n",
    "from iomeval.mapper import map_srf_enablers, map_srf_crosscutting, map_gcm, map_srf_outputs\n",
    "\n",
    "# Each mapper returns structured results with centrality scores\n",
    "report.srf_enablers = map_srf_enablers(report.conclusions)\n",
    "report.srf_crosscutting = map_srf_crosscutting(report.conclusions)\n",
    "report.gcm = map_gcm(report.conclusions)\n",
    "report.srf_outputs = map_srf_outputs(report.conclusions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5bd314",
   "metadata": {},
   "source": [
    "### 5. Save/restore checkpoints\n",
    "\n",
    "```python\n",
    "# Save progress\n",
    "report.save(\"checkpoint.json\")\n",
    "\n",
    "# Resume later\n",
    "report = Report.load(\"checkpoint.json\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a438f4d",
   "metadata": {},
   "source": [
    "## Development\n",
    "\n",
    "iomeval is built with [nbdev](https://nbdev.fast.ai/), which means the entire library is developed in Jupyter notebooks. The notebooks serve as both documentation and source code.\n",
    "\n",
    "### Setup for development\n",
    "\n",
    "```sh\n",
    "git clone https://github.com/franckalbinet/iomeval.git\n",
    "cd iomeval\n",
    "pip install -e '.[dev]'\n",
    "```\n",
    "\n",
    "### Key nbdev commands\n",
    "\n",
    "```sh\n",
    "nbdev_test          # Run tests in notebooks\n",
    "nbdev_export        # Export notebooks to Python modules\n",
    "nbdev_preview       # Preview documentation site\n",
    "nbdev_prepare       # Export, test, and clean notebooks (run before committing)\n",
    "```\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. Make changes in the `.ipynb` notebook files\n",
    "2. Run `nbdev_prepare` to export code and run tests\n",
    "3. Commit both notebooks and exported Python files\n",
    "4. Documentation is automatically generated from the notebooks\n",
    "\n",
    "Learn more about nbdev's literate programming approach in the [nbdev documentation](https://nbdev.fast.ai/).\n",
    "\n",
    "### Contributing\n",
    "\n",
    "Contributions are welcome! Please:\n",
    "- Follow the existing notebook structure\n",
    "- Add tests using nbdev's `#| test` cells\n",
    "- Run `nbdev_prepare` before submitting PRs"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
