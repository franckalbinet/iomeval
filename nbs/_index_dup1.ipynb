{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc76840",
   "metadata": {},
   "source": [
    "# IOMEval\n",
    "> Streamline evaluation reports into structured knowledge maps with LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d187f1",
   "metadata": {},
   "source": [
    "[![CI](https://github.com/franckalbinet/iomeval/actions/workflows/test.yaml/badge.svg)](https://github.com/franckalbinet/iomeval/actions/workflows/test.yaml)\n",
    "[![PyPI](https://img.shields.io/pypi/v/iomeval)](https://pypi.org/project/iomeval/)\n",
    "[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e6e49a",
   "metadata": {},
   "source": [
    "`iomeval` streamlines the mapping of [IOM](https://www.iom.int) evaluation reports against strategic frameworks like the [Strategic Results Framework (SRF)](https://srf.iom.int/) and [Global Compact for Migration (GCM)](https://www.un.org/en/development/desa/population/migration/generalassembly/docs/globalcompact/A_RES_73_195.pdf). It uses LLMs to process PDF reports, extract key sections, and tag/map them to framework components, turning dispersed, untagged evaluation documents into structured, searchable knowledge maps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12ae036",
   "metadata": {},
   "source": [
    "## Why This Matters\n",
    "\n",
    "UN agencies produce extensive evaluation reports and other public documents. For IOM, this body of knowledge is extensive and variegated, but putting it to practical use becomes more challenging as volume increases, particularly when documentation is stored across different repositories with no single index available.\n",
    "\n",
    "**The Challenge for IOM**\n",
    "\n",
    "IOM's evaluation production is highly decentralized, with reports stored across multiple repositories (the [IOM Evaluation Repository](https://evaluation.iom.int/evaluation-search-pdf), IOM Library, IOM Protection Platform). Quality varies greatly: quality control processes are not applied uniformly, and variation also reflects the inherent subjectivity in evaluation approaches and interpretations. Reports also vary significantly in structure: some follow common formats with executive summaries, findings, and recommendations, while others have different structures entirely. This inconsistency makes systematic mapping challenging.\n",
    "\n",
    "Critically, existing metadata doesn't indicate which elements of IOM's strategic frameworks (the [Strategic Results Framework (SRF)](https://srf.iom.int/) or the [Global Compact for Migration (GCM)](https://www.un.org/en/development/desa/population/migration/generalassembly/docs/globalcompact/A_RES_73_195.pdf)) each report addresses. This is a major gap that limits the ability to connect evaluation evidence with key strategic frameworks.\n",
    "\n",
    "**Evidence Maps as a Solution**\n",
    "\n",
    "Evidence Maps display the extent and nature of research and evaluation available on a subject. Following the [2025 UNEG Eval Week](https://www.unevaluation.org/events/uneg-evaluation-week-2025), four primary use cases emerged: guiding future evidence generation, informing policy decisions, knowledge management, and enhancing collaboration. The maps created by `iomeval` serve primarily as **knowledge management tools**â€”structured repositories that make identifying relevant sources easier by organizing them against strategic framework components.\n",
    "\n",
    "**What This Enables**\n",
    "\n",
    "By tagging reports against SRF outputs, enablers, cross-cutting priorities, and GCM objectives, these maps help answer questions like: Which framework elements are well-covered by existing evaluations? Where are the knowledge gaps that should prioritize future evaluation work? Which themes have enough evidence for a dedicated synthesis report?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1814f336",
   "metadata": {},
   "source": [
    "## Key Features\n",
    "\n",
    "- **Automated PDF Processing**: Download and OCR evaluation reports with proper heading hierarchy\n",
    "- **Intelligent Section Extraction**: LLM-powered extraction of executive summaries, findings, conclusions, and recommendations\n",
    "- **Strategic Framework Mapping**: Map report content to IOM's SRF Enablers, Cross-Cutting Priorities, GCM Objectives, and SRF Outputs\n",
    "- **Checkpoint/Resume**: Built-in state persistence - interrupt and resume long-running pipelines\n",
    "- **Granular Control**: Use the full pipeline or individual components as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9441fdd8",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install from PyPI:\n",
    "```sh\n",
    "pip install iomeval\n",
    "```\n",
    "\n",
    "Or install the latest development version from GitHub:\n",
    "```sh\n",
    "pip install git+https://github.com/franckalbinet/iomeval.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f48f0",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "### Core Dependencies\n",
    "\n",
    "iomeval relies on two key libraries:\n",
    "\n",
    "- **[mistocr](https://fr.anckalbi.net/mistocr)**: Powers the PDF-to-markdown conversion with intelligent OCR and heading hierarchy detection\n",
    "- **[lisette](https://lisette.answer.ai)**: A thin wrapper around [litellm](https://www.litellm.ai/) that provides access to all major LLM providers. By default, iomeval uses Anthropic models (Haiku for debugging, Sonnet for production)\n",
    "\n",
    "### API Keys\n",
    "\n",
    "`iomeval` automatically loads API keys on import. You have two options:\n",
    "\n",
    "**Option 1: Environment variables** (recommended for production)\n",
    "```sh\n",
    "export ANTHROPIC_API_KEY='your-key-here'\n",
    "export MISTRAL_API_KEY='your-key-here'\n",
    "```\n",
    "\n",
    "**Option 2: .env file** (convenient for development)\n",
    "\n",
    "Create a `.env` file in your project root:\n",
    "\n",
    "```\n",
    "ANTHROPIC_API_KEY=your-key-here\n",
    "MISTRAL_API_KEY=your-key-here\n",
    "```\n",
    "\n",
    "Since lisette supports all major LLM providers via litellm, you can configure other providers (OpenAI, Google, etc.) by setting their respective API keys using either method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58acdbb3",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "\n",
    "First, prepare your evaluations data. Export evaluations from [IOM's evaluation repository](https://evaluation.iom.int/evaluation-search-pdf) as CSV, then convert to JSON:\n",
    "\n",
    "```python\n",
    "from iomeval.readers import IOMRepoReader\n",
    "\n",
    "reader = IOMRepoReader('evaluation-search-export.csv')\n",
    "reader.to_json('evaluations.json')\n",
    "```\n",
    "\n",
    "Now process an evaluation report end-to-end:\n",
    "\n",
    "```python\n",
    "from iomeval.readers import load_evals\n",
    "from iomeval.pipeline import run_pipeline\n",
    "\n",
    "evals = load_evals('evaluations.json')\n",
    "url = \"https://evaluation.iom.int/sites/g/files/tmzbdl151/files/docs/resources/Abridged%20Evaluation%20Report_%20Final_Olta%20NDOJA.pdf\"\n",
    "\n",
    "report = await run_pipeline(url, evals, \n",
    "                            pdf_dst='data/pdfs', \n",
    "                            md_dst='data/md', \n",
    "                            results_path='data/results', \n",
    "                            ocr_kwargs=dict(add_img_desc=False), \n",
    "                            model='claude-haiku-4-5')\n",
    "report\n",
    "```\n",
    "\n",
    "The pipeline runs 7 steps: \n",
    "\n",
    "**download â†’ OCR â†’ extract â†’ map SRF Enablers â†’ map SRF Cross-cutting Priorities â†’ map GCM Objectives â†’ map SRF Outputs** \n",
    "\n",
    "Progress is displayed as each step completes, and state is automatically saved after each stage for checkpoint/resume capability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c424614",
   "metadata": {},
   "source": [
    "## Detailed Workflow\n",
    "\n",
    "For more control over individual pipeline stages, see the module documentation:\n",
    "\n",
    "- **Loading evaluation metadata**: See [readers](https://fr.anckalbi.net/iomeval/readers.html) for working with IOM evaluation data\n",
    "- **Downloading and OCR**: See [downloaders](https://fr.anckalbi.net/iomeval/downloaders.html) and [core](https://fr.anckalbi.net/iomeval/core.html) for PDF processing\n",
    "- **Section extraction**: See [extract](https://fr.anckalbi.net/iomeval/extract.html) for extracting executive summaries, findings, conclusions, and recommendations\n",
    "- **Framework mapping**: See [mapper](https://fr.anckalbi.net/iomeval/mapper.html) for mapping to SRF enablers, cross-cutting priorities, GCM objectives, and SRF outputs\n",
    "- **Pipeline control**: See [pipeline](https://fr.anckalbi.net/iomeval/pipeline.html) for granular control over the full pipeline and checkpoint/resume functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a438f4d",
   "metadata": {},
   "source": [
    "## Development\n",
    "\n",
    "iomeval is built with [nbdev](https://nbdev.fast.ai/), which means the entire library is developed in Jupyter notebooks. The notebooks serve as both documentation and source code.\n",
    "\n",
    "### Setup for development\n",
    "\n",
    "```sh\n",
    "git clone https://github.com/franckalbinet/iomeval.git\n",
    "cd iomeval\n",
    "pip install -e '.[dev]'\n",
    "```\n",
    "\n",
    "### Key nbdev commands\n",
    "\n",
    "```sh\n",
    "nbdev_test          # Run tests in notebooks\n",
    "nbdev_export        # Export notebooks to Python modules\n",
    "nbdev_preview       # Preview documentation site\n",
    "nbdev_prepare       # Export, test, and clean notebooks (run before committing)\n",
    "```\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. Make changes in the `.ipynb` notebook files\n",
    "2. Run `nbdev_prepare` to export code and run tests\n",
    "3. Commit both notebooks and exported Python files\n",
    "4. Documentation is automatically generated from the notebooks\n",
    "\n",
    "Learn more about nbdev's literate programming approach in the [nbdev documentation](https://nbdev.fast.ai/).\n",
    "\n",
    "### Contributing\n",
    "\n",
    "Contributions are welcome! Please:\n",
    "- Follow the existing notebook structure\n",
    "- Run `nbdev_prepare` before submitting PRs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a95bf86",
   "metadata": {},
   "source": [
    "After having run `pip install -U` how to guarantee we have the latest version of my package. In Vscode, I have update a package but still using the previous version. Is there somethign related to cache, ...?\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "This is a common frustration! Even after `pip install -U packagename`, you might still be using the old version. Here are the main culprits:\n",
    "\n",
    "**1. Python kernel/interpreter hasn't restarted**\n",
    "   - In VSCode, your Python interpreter is still running with the old version loaded in memory\n",
    "   - Solution: Restart your Python kernel (if using Jupyter) or restart your Python interpreter\n",
    "\n",
    "**2. Import cache**\n",
    "   - Python caches imported modules in `sys.modules`\n",
    "   - Solution: If you can't restart, try:\n",
    "     ```python\n",
    "     import sys\n",
    "     if 'iomeval' in sys.modules:\n",
    "         del sys.modules['iomeval']\n",
    "     import iomeval\n",
    "     ```\n",
    "   - But honestly, **restarting is cleaner and more reliable**\n",
    "\n",
    "**3. Verify what's actually installed**\n",
    "   ```python\n",
    "   import iomeval\n",
    "   print(iomeval.__version__)  # Check version\n",
    "   print(iomeval.__file__)     # Check where it's loading from\n",
    "   ```\n",
    "\n",
    "**4. Multiple Python environments**\n",
    "   - You might have updated the package in one environment but VSCode is using another\n",
    "   - Check which Python VSCode is using (bottom right corner in VSCode)\n",
    "   - Verify: `!which python` or `!where python` (Windows)\n",
    "\n",
    "**5. Development install (`pip install -e`)**\n",
    "   - If you installed with `-e` (editable mode), pip won't update it\n",
    "   - You need to uninstall first: `pip uninstall iomeval`, then reinstall\n",
    "\n",
    "What's your situation? Are you developing iomeval yourself (using `-e` install) or just using it as a regular package?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06348e5e",
   "metadata": {},
   "source": [
    "How to put the code below:\n",
    "\n",
    "```\n",
    "from pathlib import Path\n",
    "from iomeval.readers import IOMRepoReader\n",
    "\n",
    "data_dir = Path('data')\n",
    "evl_repo = data_dir / 'evaluation-search-export-12_10_2025--18_07_30.csv'\n",
    "reader = IOMRepoReader(evl_repo)\n",
    "reader.to_json(data_dir/'evaluations.json')\n",
    "\n",
    "from iomeval.readers import load_evals\n",
    "from iomeval.pipeline import run_pipeline\n",
    "\n",
    "evals = load_evals(data_dir/'evaluations.json')\n",
    "\n",
    "url = \"https://evaluation.iom.int/sites/g/files/tmzbdl151/files/docs/resources/Abridged%20Evaluation%20Report_%20Final_Olta%20NDOJA.pdf\"\n",
    "\n",
    "report = await run_pipeline(url, evals, \n",
    "                            pdf_dst='data/pdfs', \n",
    "                            md_dst='data/md', \n",
    "                            results_path='data/results', \n",
    "                            ocr_kwargs=dict(add_img_desc=False), \n",
    "                            model='claude-haiku-4-5')\n",
    "```\n",
    "\n",
    "in a python file I can run (considereing the await)\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "To run code with `await` in a regular Python file, you have a few options:\n",
    "\n",
    "**Option 1: Wrap in an async function and use `asyncio.run()`**\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from iomeval.readers import IOMRepoReader, load_evals\n",
    "from iomeval.pipeline import run_pipeline\n",
    "\n",
    "async def main():\n",
    "    data_dir = Path('data')\n",
    "    evl_repo = data_dir / 'evaluation-search-export-12_10_2025--18_07_30.csv'\n",
    "    reader = IOMRepoReader(evl_repo)\n",
    "    reader.to_json(data_dir/'evaluations.json')\n",
    "    \n",
    "    evals = load_evals(data_dir/'evaluations.json')\n",
    "    url = \"https://evaluation.iom.int/sites/g/files/tmzbdl151/files/docs/resources/Abridged%20Evaluation%20Report_%20Final_Olta%20NDOJA.pdf\"\n",
    "    \n",
    "    report = await run_pipeline(url, evals, \n",
    "                                pdf_dst='data/pdfs', \n",
    "                                md_dst='data/md', \n",
    "                                results_path='data/results', \n",
    "                                ocr_kwargs=dict(add_img_desc=False), \n",
    "                                model='claude-haiku-4-5')\n",
    "    return report\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    report = asyncio.run(main())\n",
    "    print(report)\n",
    "```\n",
    "\n",
    "**Option 2: Use `nest_asyncio`** (if running in Jupyter/IPython-like environments)\n",
    "\n",
    "This is simpler but only needed if you get event loop errors.\n",
    "\n",
    "Which approach would you like to use? Are you running this as a standalone script or in a notebook environment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2768b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../iomeval/iomeval:\r\n",
      "__init__.py  _modidx.py  downloaders.py  files\t    pipeline.py  themes.py\r\n",
      "__pycache__  core.py\t extract.py\t mapper.py  readers.py\r\n",
      "\r\n",
      "../../iomeval/iomeval/__pycache__:\r\n",
      "__init__.cpython-312.pyc  downloaders.cpython-312.pyc  readers.cpython-312.pyc\r\n",
      "_modidx.cpython-312.pyc   extract.cpython-312.pyc      themes.cpython-312.pyc\r\n",
      "core.cpython-312.pyc\t  mapper.cpython-312.pyc\r\n",
      "\r\n",
      "../../iomeval/iomeval/files:\r\n",
      "prompts  themes\r\n",
      "\r\n",
      "../../iomeval/iomeval/files/prompts:\r\n",
      "gcm.md\tselect_sections.md  srf_ccps.md  srf_enablers.md  srf_outputs.md\r\n",
      "\r\n",
      "../../iomeval/iomeval/files/themes:\r\n",
      "crosscutting_priorities.json  gcms_short.md\t   srf_outputs_to_gcm.json\r\n",
      "gcm_to_srf_outputs.json       srf_enablers.json\r\n",
      "gcms_long.md\t\t      srf_objectives.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls -R ../../iomeval/iomeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edf73ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../iomeval/nbs:\r\n",
      "00_core.ipynb\t      04_themes.ipynb\t files\t      sidebar.yml\r\n",
      "01_readers.ipynb      05_mapper.ipynb\t index.ipynb  styles.css\r\n",
      "02_downloaders.ipynb  06_pipeline.ipynb  nbdev.yml\r\n",
      "03_extract.ipynb      _quarto.yml\t ocr_temp\r\n",
      "\r\n",
      "../../iomeval/nbs/files:\r\n",
      "glossaries  prompts  test  themes\r\n",
      "\r\n",
      "../../iomeval/nbs/files/glossaries:\r\n",
      "migration.json\r\n",
      "\r\n",
      "../../iomeval/nbs/files/prompts:\r\n",
      "gcm.md\tselect_sections.md  srf_ccps.md  srf_enablers.md  srf_outputs.md\r\n",
      "\r\n",
      "../../iomeval/nbs/files/test:\r\n",
      "AAP%20Evaluation%20Report_final_\t\t   evaluations.json\tpdfs\r\n",
      "eval_repo_iom.xlsx\t\t\t\t   image_cache.json\tresults\r\n",
      "eval_reports\t\t\t\t\t   iom_evals_test.json\r\n",
      "evaluation-search-export-11_13_2025--18_09_44.csv  md\r\n",
      "\r\n",
      "../../iomeval/nbs/files/test/AAP%20Evaluation%20Report_final_:\r\n",
      "img\t    page_23.md\tpage_38.md  page_52.md\tpage_67.md  page_81.md\r\n",
      "page_1.md   page_24.md\tpage_39.md  page_53.md\tpage_68.md  page_82.md\r\n",
      "page_10.md  page_25.md\tpage_4.md   page_54.md\tpage_69.md  page_83.md\r\n",
      "page_11.md  page_26.md\tpage_40.md  page_55.md\tpage_7.md   page_84.md\r\n",
      "page_12.md  page_27.md\tpage_41.md  page_56.md\tpage_70.md  page_85.md\r\n",
      "page_13.md  page_28.md\tpage_42.md  page_57.md\tpage_71.md  page_86.md\r\n",
      "page_14.md  page_29.md\tpage_43.md  page_58.md\tpage_72.md  page_87.md\r\n",
      "page_15.md  page_3.md\tpage_44.md  page_59.md\tpage_73.md  page_88.md\r\n",
      "page_16.md  page_30.md\tpage_45.md  page_6.md\tpage_74.md  page_89.md\r\n",
      "page_17.md  page_31.md\tpage_46.md  page_60.md\tpage_75.md  page_9.md\r\n",
      "page_18.md  page_32.md\tpage_47.md  page_61.md\tpage_76.md  page_90.md\r\n",
      "page_19.md  page_33.md\tpage_48.md  page_62.md\tpage_77.md  page_91.md\r\n",
      "page_2.md   page_34.md\tpage_49.md  page_63.md\tpage_78.md  page_92.md\r\n",
      "page_20.md  page_35.md\tpage_5.md   page_64.md\tpage_79.md  page_93.md\r\n",
      "page_21.md  page_36.md\tpage_50.md  page_65.md\tpage_8.md   page_94.md\r\n",
      "page_22.md  page_37.md\tpage_51.md  page_66.md\tpage_80.md  page_95.md\r\n",
      "\r\n",
      "../../iomeval/nbs/files/test/AAP%20Evaluation%20Report_final_/img:\r\n",
      "img-0.jpeg   img-12.jpeg  img-16.jpeg  img-3.jpeg  img-7.jpeg\r\n",
      "img-1.jpeg   img-13.jpeg  img-17.jpeg  img-4.jpeg  img-8.jpeg\r\n",
      "img-10.jpeg  img-14.jpeg  img-18.jpeg  img-5.jpeg  img-9.jpeg\r\n",
      "img-11.jpeg  img-15.jpeg  img-2.jpeg   img-6.jpeg\r\n",
      "\r\n",
      "../../iomeval/nbs/files/test/eval_reports:\r\n",
      "2b297bc904ed1e542fbe67f62a8ea7cf  9992310969aa2f428bc8aba29f865cf3\r\n",
      "8faad6edb0414cfc8f35415a93726e36\r\n",
      "\r\n",
      "../../iomeval/nbs/files/test/eval_reports/2b297bc904ed1e542fbe67f62a8ea7cf:\r\n",
      "Meta-Evaluation-Infographics.pdf\r\n",
      "Meta-evaluation%202020-2024%20-%20Evaluation%20Brief_0.pdf\r\n",
      "Meta-evaluation%202020-2024%20report%20-%20clean.pdf\r\n",
      "\r\n",
      "../../iomeval/nbs/files/test/eval_reports/8faad6edb0414cfc8f35415a93726e36:\r\n",
      "CS1137-CS1138_Evaluation%20Brief_Javier%20SANCHEZ%20MARTI.pdf\r\n",
      "CS1137-CS1138_Evaluation%20ToR_Javier%20SANCHEZ%20MARTI.pdf\r\n",
      "CS1137-CS1138_Final%20Report_French_Javier%20SANCHEZ%20MARTI.pdf\r\n",
      "CS1137-CS1138_Final%20Report_Spanish_Javier%20SANCHEZ%20MARTI.pdf\r\n",
      "CS1137-CS1138_Inception%20Report_Javier%20SANCHEZ%20MARTI.pdf\r\n",
      "CS1137-CS1138_Management%20Response%20Plan_Javier%20SANCHEZ%20MARTI.pdf\r\n",
      "\r\n",
      "../../iomeval/nbs/files/test/eval_reports/9992310969aa2f428bc8aba29f865cf3:\r\n",
      "Annex%20VI%20Case%20Study%20-%20RDH%20East%2C%20Horn%20and%20Southern%20Africa.pdf\r\n",
      "Annex%20VII%20Case%20Study%20-%20RDH%20Asia-Pacific.pdf\r\n",
      "\r\n",
      "../../iomeval/nbs/files/test/md:\r\n",
      "49d2fba781b6a7c0d94577479636ee6f  6c3c2cf3fa479112967612b0baddab72\r\n",
      "\r\n",
      "../../iomeval/nbs/files/test/md/49d2fba781b6a7c0d94577479636ee6f:\r\n",
      "img\t\t       page_14.md  page_20.md  page_27.md  page_5.md\r\n",
      "img_descriptions.json  page_15.md  page_21.md  page_28.md  page_6.md\r\n",
      "page_1.md\t       page_16.md  page_22.md  page_29.md  page_7.md\r\n",
      "page_10.md\t       page_17.md  page_23.md  page_3.md   page_8.md\r\n",
      "page_11.md\t       page_18.md  page_24.md  page_30.md  page_9.md\r\n",
      "page_12.md\t       page_19.md  page_25.md  page_31.md\r\n",
      "page_13.md\t       page_2.md   page_26.md  page_4.md\r\n",
      "\r\n",
      "../../iomeval/nbs/files/test/md/49d2fba781b6a7c0d94577479636ee6f/img:\r\n",
      "img-0.jpeg  img-2.jpeg\timg-4.jpeg  img-6.jpeg\r\n",
      "img-1.jpeg  img-3.jpeg\timg-5.jpeg\r\n",
      "\r\n",
      "../../iomeval/nbs/files/test/md/6c3c2cf3fa479112967612b0baddab72:\r\n",
      "img\t    page_23.md\tpage_38.md  page_52.md\tpage_67.md  page_81.md\r\n",
      "page_1.md   page_24.md\tpage_39.md  page_53.md\tpage_68.md  page_82.md\r\n",
      "page_10.md  page_25.md\tpage_4.md   page_54.md\tpage_69.md  page_83.md\r\n",
      "page_11.md  page_26.md\tpage_40.md  page_55.md\tpage_7.md   page_84.md\r\n",
      "page_12.md  page_27.md\tpage_41.md  page_56.md\tpage_70.md  page_85.md\r\n",
      "page_13.md  page_28.md\tpage_42.md  page_57.md\tpage_71.md  page_86.md\r\n",
      "page_14.md  page_29.md\tpage_43.md  page_58.md\tpage_72.md  page_87.md\r\n",
      "page_15.md  page_3.md\tpage_44.md  page_59.md\tpage_73.md  page_88.md\r\n",
      "page_16.md  page_30.md\tpage_45.md  page_6.md\tpage_74.md  page_89.md\r\n",
      "page_17.md  page_31.md\tpage_46.md  page_60.md\tpage_75.md  page_9.md\r\n",
      "page_18.md  page_32.md\tpage_47.md  page_61.md\tpage_76.md  page_90.md\r\n",
      "page_19.md  page_33.md\tpage_48.md  page_62.md\tpage_77.md  page_91.md\r\n",
      "page_2.md   page_34.md\tpage_49.md  page_63.md\tpage_78.md  page_92.md\r\n",
      "page_20.md  page_35.md\tpage_5.md   page_64.md\tpage_79.md  page_93.md\r\n",
      "page_21.md  page_36.md\tpage_50.md  page_65.md\tpage_8.md   page_94.md\r\n",
      "page_22.md  page_37.md\tpage_51.md  page_66.md\tpage_80.md  page_95.md\r\n",
      "\r\n",
      "../../iomeval/nbs/files/test/md/6c3c2cf3fa479112967612b0baddab72/img:\r\n",
      "img-0.jpeg   img-12.jpeg  img-16.jpeg  img-3.jpeg  img-7.jpeg\r\n",
      "img-1.jpeg   img-13.jpeg  img-17.jpeg  img-4.jpeg  img-8.jpeg\r\n",
      "img-10.jpeg  img-14.jpeg  img-18.jpeg  img-5.jpeg  img-9.jpeg\r\n",
      "img-11.jpeg  img-15.jpeg  img-2.jpeg   img-6.jpeg\r\n",
      "\r\n",
      "../../iomeval/nbs/files/test/pdfs:\r\n",
      "49d2fba781b6a7c0d94577479636ee6f  6c3c2cf3fa479112967612b0baddab72\r\n",
      "\r\n",
      "../../iomeval/nbs/files/test/pdfs/49d2fba781b6a7c0d94577479636ee6f:\r\n",
      "Abridged%20Evaluation%20Report_%20Final_Olta%20NDOJA.pdf\r\n",
      "Evaluation%20Learning%20Brief_Final_Olta%20NDOJA.pdf\r\n",
      "Final%20Evaluation%20Report%20Final_Olta%20NDOJA.pdf\r\n",
      "HoA%20EU%20JI%20Final%20Eval%20-%20Management%20Response%20Matrix%20-%20Final.pdf\r\n",
      "ISP_IOM_Case-Management-Return-Reintegr-JI-Review_final.pdf\r\n",
      "\r\n",
      "../../iomeval/nbs/files/test/pdfs/6c3c2cf3fa479112967612b0baddab72:\r\n",
      "AAP%20Evaluation%20-%20Management%20Response%20Final.docx\r\n",
      "AAP%20Evaluation%20Report_final_.pdf\r\n",
      "AAP%20Infographics.pdf\r\n",
      "AAP_EN_Evaluation%20Brief_final_0.pdf\r\n",
      "\r\n",
      "../../iomeval/nbs/files/test/results:\r\n",
      "49d2fba781b6a7c0d94577479636ee6f.json  6c3c2cf3fa479112967612b0baddab72.json\r\n",
      "\r\n",
      "../../iomeval/nbs/files/themes:\r\n",
      "crosscutting_priorities.json  gcms_short.md\t   srf_outputs_to_gcm.json\r\n",
      "gcm_to_srf_outputs.json       srf_enablers.json\r\n",
      "gcms_long.md\t\t      srf_objectives.json\r\n",
      "\r\n",
      "../../iomeval/nbs/ocr_temp:\r\n",
      "AAP%20Evaluation%20Report_final_\r\n",
      "Abridged%20Evaluation%20Report_%20Final_Olta%20NDOJA\r\n",
      "Evaluation%20Learning%20Brief_Final_Olta%20NDOJA\r\n",
      "\r\n",
      "../../iomeval/nbs/ocr_temp/AAP%20Evaluation%20Report_final_:\r\n",
      "img\t\t       page_26.md  page_44.md  page_62.md  page_80.md\r\n",
      "img_descriptions.json  page_27.md  page_45.md  page_63.md  page_81.md\r\n",
      "page_1.md\t       page_28.md  page_46.md  page_64.md  page_82.md\r\n",
      "page_10.md\t       page_29.md  page_47.md  page_65.md  page_83.md\r\n",
      "page_11.md\t       page_3.md   page_48.md  page_66.md  page_84.md\r\n",
      "page_12.md\t       page_30.md  page_49.md  page_67.md  page_85.md\r\n",
      "page_13.md\t       page_31.md  page_5.md   page_68.md  page_86.md\r\n",
      "page_14.md\t       page_32.md  page_50.md  page_69.md  page_87.md\r\n",
      "page_15.md\t       page_33.md  page_51.md  page_7.md   page_88.md\r\n",
      "page_16.md\t       page_34.md  page_52.md  page_70.md  page_89.md\r\n",
      "page_17.md\t       page_35.md  page_53.md  page_71.md  page_9.md\r\n",
      "page_18.md\t       page_36.md  page_54.md  page_72.md  page_90.md\r\n",
      "page_19.md\t       page_37.md  page_55.md  page_73.md  page_91.md\r\n",
      "page_2.md\t       page_38.md  page_56.md  page_74.md  page_92.md\r\n",
      "page_20.md\t       page_39.md  page_57.md  page_75.md  page_93.md\r\n",
      "page_21.md\t       page_4.md   page_58.md  page_76.md  page_94.md\r\n",
      "page_22.md\t       page_40.md  page_59.md  page_77.md  page_95.md\r\n",
      "page_23.md\t       page_41.md  page_6.md   page_78.md\r\n",
      "page_24.md\t       page_42.md  page_60.md  page_79.md\r\n",
      "page_25.md\t       page_43.md  page_61.md  page_8.md\r\n",
      "\r\n",
      "../../iomeval/nbs/ocr_temp/AAP%20Evaluation%20Report_final_/img:\r\n",
      "img-0.jpeg   img-12.jpeg  img-16.jpeg  img-3.jpeg  img-7.jpeg\r\n",
      "img-1.jpeg   img-13.jpeg  img-17.jpeg  img-4.jpeg  img-8.jpeg\r\n",
      "img-10.jpeg  img-14.jpeg  img-18.jpeg  img-5.jpeg  img-9.jpeg\r\n",
      "img-11.jpeg  img-15.jpeg  img-2.jpeg   img-6.jpeg\r\n",
      "\r\n",
      "../../iomeval/nbs/ocr_temp/Abridged%20Evaluation%20Report_%20Final_Olta%20NDOJA:\r\n",
      "img\t\t       page_14.md  page_20.md  page_27.md  page_5.md\r\n",
      "img_descriptions.json  page_15.md  page_21.md  page_28.md  page_6.md\r\n",
      "page_1.md\t       page_16.md  page_22.md  page_29.md  page_7.md\r\n",
      "page_10.md\t       page_17.md  page_23.md  page_3.md   page_8.md\r\n",
      "page_11.md\t       page_18.md  page_24.md  page_30.md  page_9.md\r\n",
      "page_12.md\t       page_19.md  page_25.md  page_31.md\r\n",
      "page_13.md\t       page_2.md   page_26.md  page_4.md\r\n",
      "\r\n",
      "../../iomeval/nbs/ocr_temp/Abridged%20Evaluation%20Report_%20Final_Olta%20NDOJA/img:\r\n",
      "img-0.jpeg  img-2.jpeg\timg-4.jpeg  img-6.jpeg\r\n",
      "img-1.jpeg  img-3.jpeg\timg-5.jpeg\r\n",
      "\r\n",
      "../../iomeval/nbs/ocr_temp/Evaluation%20Learning%20Brief_Final_Olta%20NDOJA:\r\n",
      "page_1.md  page_2.md\r\n"
     ]
    }
   ],
   "source": [
    "!ls -R ../../iomeval/nbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1895076",
   "metadata": {},
   "source": [
    "settings.ini\n",
    "\n",
    "```\n",
    "[DEFAULT]\n",
    "repo = iomeval\n",
    "lib_name = iomeval\n",
    "version = 0.0.7\n",
    "min_python = 3.9\n",
    "license = apache2\n",
    "black_formatting = False\n",
    "doc_path = _docs\n",
    "lib_path = iomeval\n",
    "nbs_path = nbs\n",
    "recursive = True\n",
    "tst_flags = notest\n",
    "put_version_in_init = True\n",
    "update_pyproject = True\n",
    "branch = main\n",
    "custom_sidebar = True\n",
    "doc_host = https://franckalbinet.github.io\n",
    "doc_baseurl = /iomeval\n",
    "git_url = https://github.com/franckalbinet/iomeval\n",
    "title = iomeval\n",
    "audience = Developers\n",
    "author = Solveit\n",
    "author_email = nobody@fast.ai\n",
    "copyright = 2025 onwards, Solveit\n",
    "description = Streamline evaluation evidence mapping at scale with LLMs\n",
    "keywords = nbdev jupyter notebook python\n",
    "language = English\n",
    "status = 3\n",
    "user = franckalbinet\n",
    "requirements = fastcore pandas lisette pydantic tiktoken mistocr python-dotenv\n",
    "package_data = files/prompts/*.md files/themes/*.json files/themes/*.md\n",
    "readme_nb = index.ipynb\n",
    "allowed_metadata_keys = \n",
    "allowed_cell_metadata_keys = \n",
    "jupyter_hooks = False\n",
    "clean_ids = True\n",
    "clear_all = False\n",
    "cell_number = True\n",
    "skip_procs = \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5779f7c",
   "metadata": {},
   "source": [
    "setup.py\n",
    "```\n",
    "from pkg_resources import parse_version\n",
    "from configparser import ConfigParser\n",
    "import setuptools, shlex\n",
    "assert parse_version(setuptools.__version__)>=parse_version('36.2')\n",
    "\n",
    "# note: all settings are in settings.ini; edit there, not here\n",
    "config = ConfigParser(delimiters=['='])\n",
    "config.read('settings.ini', encoding='utf-8')\n",
    "cfg = config['DEFAULT']\n",
    "\n",
    "cfg_keys = 'version description keywords author author_email'.split()\n",
    "expected = cfg_keys + \"lib_name user branch license status min_python audience language\".split()\n",
    "for o in expected: assert o in cfg, \"missing expected setting: {}\".format(o)\n",
    "setup_cfg = {o:cfg[o] for o in cfg_keys}\n",
    "\n",
    "licenses = {\n",
    "    'apache2': ('Apache Software License 2.0','OSI Approved :: Apache Software License'),\n",
    "    'mit': ('MIT License', 'OSI Approved :: MIT License'),\n",
    "    'gpl2': ('GNU General Public License v2', 'OSI Approved :: GNU General Public License v2 (GPLv2)'),\n",
    "    'gpl3': ('GNU General Public License v3', 'OSI Approved :: GNU General Public License v3 (GPLv3)'),\n",
    "    'bsd3': ('BSD License', 'OSI Approved :: BSD License'),\n",
    "}\n",
    "statuses = [ '1 - Planning', '2 - Pre-Alpha', '3 - Alpha',\n",
    "    '4 - Beta', '5 - Production/Stable', '6 - Mature', '7 - Inactive' ]\n",
    "py_versions = '3.6 3.7 3.8 3.9 3.10 3.11 3.12'.split()\n",
    "\n",
    "requirements = shlex.split(cfg.get('requirements', ''))\n",
    "if cfg.get('pip_requirements'): requirements += shlex.split(cfg.get('pip_requirements', ''))\n",
    "min_python = cfg['min_python']\n",
    "lic = licenses.get(cfg['license'].lower(), (cfg['license'], None))\n",
    "dev_requirements = (cfg.get('dev_requirements') or '').split()\n",
    "\n",
    "package_data = dict()\n",
    "pkg_data = cfg.get('package_data', None)\n",
    "if pkg_data:\n",
    "    package_data[cfg['lib_name']] =  pkg_data.split() # split as multiple files might be listed\n",
    "# Add package data to setup_cfg for setuptools.setup(..., **setup_cfg)\n",
    "setup_cfg['package_data'] = package_data\n",
    "\n",
    "print(setup_cfg['package_data'])\n",
    "\n",
    "setuptools.setup(\n",
    "    name = cfg['lib_name'],\n",
    "    license = lic[0],\n",
    "    classifiers = [\n",
    "        'Development Status :: ' + statuses[int(cfg['status'])],\n",
    "        'Intended Audience :: ' + cfg['audience'].title(),\n",
    "        'Natural Language :: ' + cfg['language'].title(),\n",
    "    ] + ['Programming Language :: Python :: '+o for o in py_versions[py_versions.index(min_python):]] + (['License :: ' + lic[1] ] if lic[1] else []),\n",
    "    url = cfg['git_url'],\n",
    "    packages = setuptools.find_packages(),\n",
    "    include_package_data = True,\n",
    "    install_requires = requirements,\n",
    "    extras_require={ 'dev': dev_requirements },\n",
    "    dependency_links = cfg.get('dep_links','').split(),\n",
    "    python_requires  = '>=' + cfg['min_python'],\n",
    "    long_description = open('README.md', encoding='utf-8').read(),\n",
    "    long_description_content_type = 'text/markdown',\n",
    "    zip_safe = False,\n",
    "    entry_points = {\n",
    "        'console_scripts': cfg.get('console_scripts','').split(),\n",
    "        'nbdev': [f'{cfg.get(\"lib_path\")}={cfg.get(\"lib_path\")}._modidx:d']\n",
    "    },\n",
    "    **setup_cfg)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43446906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode Skip to content\\n\\n[setuptools 80.9.0 documentation](../index.html)\\n\\n[ ![Logo](https://setuptools.p'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sut = read_url(\"https://setuptools.pypa.io/en/latest/userguide/datafiles.html\")\n",
    "sut[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3a74e",
   "metadata": {},
   "source": [
    "I am struggling with consuming local `files` when iomeval is installed locally (in a non notebook folder). Can you check the doc above to investigate why I consistently getting this error \"FileNotFoundError: [Errno 2] No such file or directory: '/Users/franckalbinet/pro/dev/iom-pipeline/.venv/lib/python3.12/site-packages/iomeval/files/prompts/select_sections.md'\"\n",
    "\n",
    "You can access python files through the &`nb_to_md` if required.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ \n",
    "\n",
    "This is a classic package data issue! Let me investigate how your code is trying to load those files. Could you point me to which module is loading these prompt files? I suspect it's in `extract.py` or `mapper.py`.\n",
    "\n",
    "Let me check the relevant notebook to see how the file paths are being constructed:\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01T9rGEB8hrPZZ1R8WRnn6wS\",\n",
    "  \"call\": {\n",
    "    \"function\": \"nb_to_md\",\n",
    "    \"arguments\": {\n",
    "      \"path\": \"../../iomeval/nbs/05_mapper.ipynb\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"'# Mapper\\\\n\\\\n> Maps IOM evaluation report against evaluation frameworks \\\\n\\\\n---\\\\n\\\\nMaps evaluation r<TRUNCATED>\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "Now let me check `core.py` to see how `load_prompt` is implemented:\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01SFSzZs5BBqeQtFC25PMjL5\",\n",
    "  \"call\": {\n",
    "    \"function\": \"nb_to_md\",\n",
    "    \"arguments\": {\n",
    "      \"path\": \"../../iomeval/nbs/00_core.ipynb\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"'# Core\\\\n\\\\n> Utilities function\\\\n\\\\n---\\\\n\\\\n```python\\\\n#| default_exp core\\\\n```\\\\n\\\\n```python\\\\n#| expor<TRUNCATED>\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "I found the issue! Look at the `load_prompt` function:\n",
    "\n",
    "```python\n",
    "def load_prompt(name:str, path:Path|str|None=None) -> str:\n",
    "    if path is None:\n",
    "        try: path = Path(__file__).parent / 'files' / 'prompts'\n",
    "        except NameError: path = Path('files/prompts')  # notebook fallback\n",
    "    return (Path(path)/f'{name}.md').read_text()\n",
    "```\n",
    "\n",
    "The path resolution uses `Path(__file__).parent / 'files' / 'prompts'`, which should work **if** the files are actually installed in the package.\n",
    "\n",
    "The problem is likely in your `settings.ini` / `setup.py`. You have:\n",
    "\n",
    "```\n",
    "package_data = files/prompts/*.md files/themes/*.json files/themes/*.md\n",
    "```\n",
    "\n",
    "But the `files` folder needs to exist **inside** `iomeval/` (the lib_path), not just in `nbs/`. \n",
    "\n",
    "Can you confirm: does `../../iomeval/iomeval/files/prompts/select_sections.md` actually exist? From your earlier `ls -R` output, I see:\n",
    "\n",
    "```\n",
    "../../iomeval/iomeval/files/prompts:\n",
    "gcm.md  select_sections.md  srf_ccps.md  srf_enablers.md  srf_outputs.md\n",
    "```\n",
    "\n",
    "So the files **do** exist in the source. The issue is they're not being included when you `pip install`. \n",
    "\n",
    "Do you have a `MANIFEST.in` file in your iomeval repo root? That's often needed to ensure non-Python files get included in the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb97ae3",
   "metadata": {},
   "source": [
    "Yes here is the MANIFEST.in\n",
    "```\n",
    "include settings.ini\n",
    "include LICENSE\n",
    "include CONTRIBUTING.md\n",
    "include README.md\n",
    "recursive-exclude * __pycache__\n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
